{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "294d7e71",
   "metadata": {},
   "source": [
    "# üìò Example: Database Builder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb431d9a",
   "metadata": {},
   "source": [
    "In this notebook we are going to create a complete **database** for our FloodAdapt model in Charleston, USA. \n",
    "To do so we use the in FloodAdapt integrated **Database builder**, which allows for a quick and easy database creation!\n",
    "We presume that you have already created a functioning [**Delft-FIAT**](https://github.com/Deltares/Delft-FIAT) and [**SFINCS model**](https://github.com/Deltares/SFINCS) and we\"ll go from there. \n",
    "\n",
    "To build a FloodAdapt database we need to set up the **configurations** for the **DatabaseBuilder**. The configuration file consists of:\n",
    "1. **Basic model parameters** to create a simple FloodAdapt database.\n",
    "2. **Advance model parameters** to create a more complex FloodAdapt database.\n",
    "\n",
    "The configuration file can be either created through the FloodAdapt objects or can we parsed as a simple dictionary.\n",
    "We advice you to work with the FloodAdapt classes, unless your database only has a few simple inputs. In this notebook we will demonstrate both options for a simple and an advanced FloodAdapt database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ce53f4",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92656339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import toml\n",
    "from pathlib import Path\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "import flood_adapt.adapter.fiat_adapter as fiat\n",
    "import flood_adapt.database_builder.database_builder as db\n",
    "from flood_adapt import FloodAdapt\n",
    "from flood_adapt.objects.forcing.tide_gauge import TideGaugeSource\n",
    "from flood_adapt.objects.forcing.timeseries import Scstype\n",
    "from flood_adapt.config.config import Settings\n",
    "from flood_adapt import unit_system as us\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9349eb11",
   "metadata": {},
   "source": [
    "## üîç **Step 1**. Explore the Delft-FIAT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617bf3b3",
   "metadata": {},
   "source": [
    "The study area is in **Charleston, USA**, a coastal city on the East Coast of the United States. \n",
    "So, let's first inspect the exposure data in the **Delft-FIAT model** to get an understanding of our study area.  \n",
    "\n",
    "We initiate the **FiatAdapter** in FloodAdapt with the model root of the Delft-FIAT model and explore the exposure data in the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa7c1830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the FiatAdapter class from FloodAdapt and plot the Fiat model. \n",
    "# Set up the settings for the database\n",
    "# Define the static data folder\n",
    "STATIC_DATA_DIR = Path(\"../../_data/examples/static-data/1_DatabaseBuilder\").resolve()\n",
    "fn_fiat = Path(STATIC_DATA_DIR  / \"fiat\") \n",
    "fa = fiat.FiatAdapter(\n",
    "    model_root =  fn_fiat\n",
    "    )\n",
    "\n",
    "# Get the geodataframe with exposure data\n",
    "gdf = fa._model.exposure.get_full_gdf(fa._model.exposure.exposure_db)\n",
    "\n",
    "# Plot the region and the secondary_object_types of the exposure data\n",
    "#m = gdf.explore(column=\"secondary_object_type\", name=\"Exposure types\")\n",
    "#m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2750e9d",
   "metadata": {},
   "source": [
    "## üìÑ **Step 2**: Build a **simple** FloodAdapt model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dc1ac9",
   "metadata": {},
   "source": [
    "In this step we will look at the basic model parameters to create a **simple** FloodAdapt model.  \n",
    "You can either create the configuration file manually with a dictionary or use the integrated classes in FloodAdapt for a more streamlined approach. \n",
    "We show you both options below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc3afd0",
   "metadata": {},
   "source": [
    "### üìö **Step 2a**: Build the configuration from the **Database Builder classes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa85c9ac",
   "metadata": {},
   "source": [
    "To create the configuration object we need to initiate the `db.ConfigModel` object.\n",
    "After we define some general attributes we need to set the GUI parameters in the `gui` attribute. The `gui` attributs define how the output is visualized in the GUI. More importantly we need to point to the Delft-FIAT and SFINCS model by providing the path in form of a `str` to the `fiat` and `sfincs_overland` attributes, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c220e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initiate the config_model for a simple FloodAdapt Database configuration file \n",
    "config_model = db.ConfigModel(\n",
    "    name=\"charleston_example1\",\n",
    "    database_path= str((STATIC_DATA_DIR /\"Database\").absolute()),\n",
    "    unit_system= db.UnitSystems.imperial,\n",
    "    gui=db.GuiConfigModel(\n",
    "        max_flood_depth=5,\n",
    "        max_aggr_dmg=1e6,\n",
    "        max_footprint_dmg=250000,\n",
    "        max_benefits=5e6,\n",
    "    ),\n",
    "    sfincs_overland=db.FloodModel(\n",
    "        name=str((STATIC_DATA_DIR  / \"overland\").absolute()),\n",
    "        reference=\"MSL\",\n",
    "    ),\n",
    "    fiat=str((STATIC_DATA_DIR / \"fiat\").absolute()),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e32de2",
   "metadata": {},
   "source": [
    "### üìñ **Step 2b**: Create a configuration file from a **dictionary**\n",
    "We can create a simple dictionary with all the attributes and save it to a configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b905e34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the configuration file for the database builder for a simple FloodAdapt database. \n",
    "# all paths should be provided with forward slashes (/)\n",
    "config = {\n",
    "\"name\": \"charleston_example2\",\n",
    "\"database_path\": \"Database\",\n",
    "\"sfincs_overland\" : { \n",
    "    \"name\":\"overland\",\n",
    "    \"reference\":\"MSL\"\n",
    "    },\n",
    "\"fiat\" : \"fiat\",\n",
    "\"unit_system\" :\"imperial\",\n",
    "\"gui\": {\n",
    "\"max_flood_depth\": 5,\n",
    "\"max_aggr_dmg\" : 10000000,\n",
    "\"max_footprint_dmg\": 250000,\n",
    "\"max_benefits\" : 50000000}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e447683b",
   "metadata": {},
   "source": [
    "When we work with a dictionary, we need to save the configuration file as a .toml-file, so that we can call it later when we run the Database Builder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365c7fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the configuration file\n",
    "with open(STATIC_DATA_DIR / \"db_config.toml\", \"w\") as f:\n",
    "    toml.dump(config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9912620",
   "metadata": {},
   "source": [
    "## üèÉ‚Äç‚ôÄÔ∏è **Step 3**: Run the Database Builder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6f119f",
   "metadata": {},
   "source": [
    "We are ready to run the Database Builder with the configuration that we just created above. First we are going to run **Option 2a** - in which we generated the configuration using the FloodAdapt classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05988d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Option 2a -  DB-builder config from FloodAdapt classes\n",
    "db_build = db.DatabaseBuilder(config_model)\n",
    "db_build.build(overwrite= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e01db8",
   "metadata": {},
   "source": [
    "Here we are going to run **Option 2b** - in which we manually created a configuration file from a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7f8576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Option 2b -  DB-builder config from dictionary\n",
    "config_path = STATIC_DATA_DIR  / \"db_config.toml\"\n",
    "config = db.ConfigModel.read(config_path)\n",
    "dbs = db.DatabaseBuilder(config)\n",
    "dbs.build(overwrite = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b348a2",
   "metadata": {},
   "source": [
    "Now you created two complete FloodAdapt Database. Both databases should be identical as we used the same inputs. You can open the databases in the GUI and explore them further or continue working with the database through the API. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5684ec6a",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è **Step 4**. Build an **advanced** FloodAdapt model\n",
    "\n",
    "In the previous step we created a simple FloodAdapt model. Our simple database is limited in functionality, so in this next step we're adding more advanced configurations to expand its capabilities so that we can create a comprehensive FloodAdapt model. \n",
    "\n",
    "The configuration file consists of:\n",
    "1. **Basic model parameters** to create a simple FloodAdapt database.\n",
    "2. **Advance model parameters** to create a more complex FloodAdapt database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608a12f7",
   "metadata": {},
   "source": [
    "### üìö **Step 2a**: Build the configuration from the **Database Builder classes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d799a92",
   "metadata": {},
   "source": [
    "First, we need to define the basic parameters like the `name` and `database_path`, together with the `unit_system` as an `UnitSystem` object and the `gui` variables, which we aggregate in the `GuiConfigModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e90805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the basic parameters\n",
    "name=\"charleston_example_advanced1\"\n",
    "database_path= str((STATIC_DATA_DIR / \"Database\").absolute())\n",
    "unit_system= db.UnitSystems.imperial\n",
    "gui=db.GuiConfigModel(\n",
    "    max_flood_depth=5,\n",
    "    max_aggr_dmg=1e6,\n",
    "    max_footprint_dmg=250000,\n",
    "    max_benefits=5e6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec45d1d",
   "metadata": {},
   "source": [
    "Additionally, to the basic model parameters we can add more parameters to make the database more complex. \n",
    "\n",
    "**Risk**   \n",
    "We can add a probabilistic event set by providing the filepath the risk event in the attribute `probabilistic_set`. If we add risk to our database we can set the `infographics` to `True`.\n",
    "We also need to define the `return_periods` in form of a list of integers or floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e37d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add risk parameters\n",
    "probabilistic_set=str(Path(STATIC_DATA_DIR  / \"test_set\"))\n",
    "infographics=True\n",
    "return_periods=[1, 2, 5, 10, 25, 50, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfb1134",
   "metadata": {},
   "source": [
    "We need to define the **sfincs model(s)** for our database. We need to add the `sfincs_overland` model by passing a dictionary with the entries: `name`, the file path,  and `reference`, the reference system (eg. \"MSL\").\n",
    " \n",
    "If we have a SFINCS offshore model we can also pass this into the configuration to the `sfincs_offshore` attribute in he same way as the overland model. For the offshore model we need to add an extra entry to the dictionary. We need to add the value and unit of the `vertical_offset`, which describes the offshore water level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835592cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the sfincs model(s)\n",
    "sfincs_overland=db.FloodModel(\n",
    "    name=str((STATIC_DATA_DIR  / \"overland\").absolute()),\n",
    "    reference=\"MSL\",\n",
    ")\n",
    "sfincs_offshore=db.FloodModel(\n",
    "    name=str(Path(STATIC_DATA_DIR / \"offshore\")),  #TODO fix final path\n",
    "    reference=\"MSL\",\n",
    "    vertical_offset=us.UnitfulLength(\n",
    "        value=0.33, units=us.UnitTypesLength.feet\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7024f7c8",
   "metadata": {},
   "source": [
    "Next we must pass a **DEM** in form of a dictionary with the entries: `filename` and `units`. The `units` should be passed as [`UnitTypesLength`](../../api_ref/UnitTypesLength.qmd) object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e709956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the DEM\n",
    "dem=db.DemModel(\n",
    "    filename=str(Path(STATIC_DATA_DIR / \"charleston_14m.tif\")), \n",
    "    units=us.UnitTypesLength.meters,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11341ac6",
   "metadata": {},
   "source": [
    "We can also exclude specific datums which we can define in the `excluded_datums` attribute. We can pass a list of datums. <span style=\"color:red;\">Why exclude datums? What does that mean in this context</span>\n",
    "\n",
    "**Important!**  <span style=\"color:red;\">double check</span>\n",
    "The water level reference should be set to the reference of your DEM. You can create a reference system manually as shown below or fetch this information from a close-by tide gauge. This will be shown in the more advanced options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fabcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the reference and exclude datums\n",
    "excluded_datums=[\"NAVD88\"]\n",
    "#references=db.WaterlevelReferenceModel(\n",
    "#    reference=\"MSL\",\n",
    "#    datums=[\n",
    "#        db.DatumModel(name=\"MSL\", height=us.UnitfulLength(value=0, units=us.UnitTypesLength.meters)),\n",
    "#        db.DatumModel(name=\"NAVD88\", height=us.UnitfulLength(value=1, units=us.UnitTypesLength.meters))\n",
    "#    ]\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec887f8",
   "metadata": {},
   "source": [
    "We can add **sea level rise scenarios** from a csv file wit the columns:  **year, unit, scenario_1, scenario_2, ..., scenario_n**. With that file we create a `SlrScenariosModel` object in which we provide the `file` and the `relative_to_year` attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac10c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add SLR scenarios\n",
    "slr_scenarios=db.SlrScenariosModel(\n",
    "    file=str(Path(STATIC_DATA_DIR  / \"slr.csv\")),\n",
    "    relative_to_year=2020,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34638334",
   "metadata": {},
   "source": [
    "Let's have a quick look what the slr scenario csv file looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f6f1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(slr_scenarios.file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85e0286",
   "metadata": {},
   "source": [
    "In a similar manner as the slr scenarios, we can add **SCS (Soil Conservation Service) rainfall** to our database by creating a `SCSModel` object, which consist of the `file` and the `type` (Scs type) attributes. The `type` should be passed in form of a `Scstype` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded53928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Soil Conservation Service rainfall\n",
    "scs=db.SCSModel(\n",
    "    file=str(Path(STATIC_DATA_DIR / \"scs_rainfall.csv\")),\n",
    "    type=Scstype.type3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b707eb60",
   "metadata": {},
   "source": [
    "Let's have a quick look what the scs scenario csv file looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5999b74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(scs.file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ab157f",
   "metadata": {},
   "source": [
    "To capture a realistic tide we can add the information from a **tide gauge**. We can either download the data from the NOAA COOPS or pass a csv file ino the `source` attribute of the `TideGaugeConfigModel`. When we download data from NOAA COOPS, we must define a `max_distance` of object type [`UnitfulLength`](../../api_ref/UnitfulLength.qmd) that describes the value (`int`) and unit ([`UnitTypesLength`](../../api_ref/UnitTypesLength.qmd)) of the maximum distance from our model domain to the nearest tide gauges to include.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5297bbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add tide gauges\n",
    "tide_gauge=db.TideGaugeConfigModel(\n",
    "    source=db.TideGaugeSource.noaa_coops,\n",
    "    max_distance=us.UnitfulLength(\n",
    "        value=100, units=us.UnitTypesLength.miles\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a048969",
   "metadata": {},
   "source": [
    "By setting the `cyclones` attribute to `True` we can add **cyclone tracks** to our database. We need to define the ocean basin we are interested in in the `cyclone_basin` attribute. The `Basins` object already has several options for us e.g. `NA` - North Atlantic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dbff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cyclone tracks\n",
    "cyclones=True\n",
    "cyclone_basin=db.Basins.NA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52753134",
   "metadata": {},
   "source": [
    "By adding **observation points** we can extract timeseries of water levels from our event scenarios. We can add a list of `ObsPointModel` objects. Each of these objects must have a `name` and a `lat`and `lon`attribute. The `description` and `ID` are optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca1599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add observation points\n",
    "obs_point=[\n",
    "    db.ObsPointModel(\n",
    "        name=\"ashley_river\",\n",
    "        description=\"Ashley River - James Island Expy\",\n",
    "        lat=32.7765,\n",
    "        lon=-79.9543,\n",
    "    ),\n",
    "    db.ObsPointModel(\n",
    "        name=8665530,\n",
    "        description=\"Charleston Cooper River Entrance\",\n",
    "        ID=8665530,\n",
    "        lat=32.78,\n",
    "        lon=-79.9233,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2391eb7c",
   "metadata": {},
   "source": [
    "We need to define the **Delft-FIAT model** for our database. All we need for that is to define the folder path that points to the Delft-FIAT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f48a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the Delft-FIAT model\n",
    "fiat=str(Path(STATIC_DATA_DIR / \"fiat\").absolute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2aabbe",
   "metadata": {},
   "source": [
    " <span style=\"color:red;\">Here I have a question - this is only used when there are NO aggregation areas in our model yet? Bc when I run it with an existing aggr layer in the model this crashes.</span>\n",
    "- `aggregation_areas`: A list of dictionaries with the entries: name, file path, field_name. Aggregates the exposure into larger spatial groups o summarize impacts on larger scale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da2be85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add aggregation areas\n",
    "#aggregation_areas=[\n",
    "#db.SpatialJoinModel(\n",
    "#    name=\"aggr_lvl_1\",\n",
    "#    file=str(\n",
    "#        Path(STATIC_DATA_DIR / aggr_lvl_1.geojson\")\n",
    "#    ),\n",
    "#    field_name=\"name\",\n",
    "#),\n",
    "#db.SpatialJoinModel(\n",
    "#    name=\"aggr_lvl_2\",\n",
    "#    file=str(Path(STATIC_DATA_DIR /aggr_lvl_2.geojson\")\n",
    "#    ),\n",
    "#    field_name=\"name\",\n",
    "#),\n",
    "#]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d43c134",
   "metadata": {},
   "source": [
    "To refine our Delft-FIAT model we can add addtional inputs or update default settings. \n",
    "\n",
    "Sometimes our Delf-FIAT model exposure is in point data By adding the `building_footprints`attribute we can download data from Open Street Map (OSM) using the `FootprintsOptions.OSM` object. e use the building footprints to create the visualizations for the impacts on the building footprint level.  \n",
    "The `fiat_buildings_name` and `fiat_roads_name` are set to *\"buildings\"* and *\"roads\"*, respectively as default. These names capture the geometry names of these assets in the Delft-FIAT model. If you specified different names for these geometry, you must change them here. By defining a value in the `road_width` attribute we can capture the realistic width of the road. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae3364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add additional Delft-FIAT parameters\n",
    "building_footprints=db.FootprintsOptions.OSM\n",
    "fiat_buildings_name=\"buildings\"\n",
    "fiat_roads_name=\"roads\"\n",
    "road_width=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ef749a",
   "metadata": {},
   "source": [
    "The base flood elevation (BFE) model allows us to elevate homes relative to the BFE. To do so we need to prvide a BFE spatial file to the `SpatialJoinModel`, defining the `name` of the spatial file, in this case `bfe` and the `field_name`, which is the column name inside the spatial file that defines the BFE value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9412e4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add base flood elevation\n",
    "bfe=db.SpatialJoinModel(\n",
    "    file=str(Path(STATIC_DATA_DIR / \"bfe.geojson\")),\n",
    "    name=\"bfe\",\n",
    "    field_name=\"bfe\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8012da",
   "metadata": {},
   "source": [
    "To capture socio-economic impacts we can add a social vulnerability (SVI) layer to the database. We need to define the `file`-path to the spatal SVI file, the `field_name` that captures the column name within the spatial file with the SVI value and the `threshold` at which point an area is classified as vulnerable.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6acfddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add social vulnerability index\n",
    "svi=db.SviConfigModel(\n",
    "    file=str(Path(STATIC_DATA_DIR / \"CDC_svi_2020.gpkg\")),\n",
    "    field_name=\"SVI\",\n",
    "    threshold=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa146ac",
   "metadata": {},
   "source": [
    "Now, that we created all the individual objects we can compile them in the `ConfigModel`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe5832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile ConfigModel\n",
    "config_model = db.ConfigModel(name = name,\n",
    "        database_path= database_path,\n",
    "        unit_system= unit_system,\n",
    "        gui=gui,\n",
    "        infographics=infographics,\n",
    "        probabilistic_set=probabilistic_set,\n",
    "        return_periods=return_periods,\n",
    "        sfincs_overland=sfincs_overland,\n",
    "        sfincs_offshore=sfincs_offshore,\n",
    "        dem=dem,\n",
    "        excluded_datums=excluded_datums,\n",
    "        #references=references,\n",
    "        slr_scenarios=slr_scenarios,\n",
    "        scs=scs,\n",
    "        tide_gauge=tide_gauge,\n",
    "        cyclones=cyclones,\n",
    "        cyclone_basin=cyclone_basin,\n",
    "        obs_point=obs_point,\n",
    "        fiat=fiat,\n",
    "        #aggregation_areas=aggregation_areas,\n",
    "        building_footprints=db.FootprintsOptions.OSM,\n",
    "        fiat_buildings_name=fiat_buildings_name,\n",
    "        fiat_roads_name=fiat_roads_name,\n",
    "        bfe=bfe,\n",
    "        svi=svi,\n",
    "        road_width=road_width,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a860c7",
   "metadata": {},
   "source": [
    "### üìñ **Step 4b**: Create a configuration file from a **dictionary**\n",
    "From verything that we learned above, we can create a simple dictionary with all the attributes and save it to a configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d1a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the configuration file for the database builder for a simple FloodAdapt database. \n",
    "\n",
    "# all paths should be provided with forward slashes (/)\n",
    "config = {\"name\": \"charleston_example_advanced2\",\n",
    " \"database_path\": \"Database\",\n",
    " \"unit_system\": \"imperial\",\n",
    " \"gui\": {\"max_flood_depth\": 5.0,\n",
    "  \"max_aggr_dmg\": 1000000.0,\n",
    "  \"max_footprint_dmg\": 250000.0,\n",
    "  \"max_benefits\": 5000000.0},\n",
    " \"infographics\": True,\n",
    " \"fiat\": \"fiat\",\n",
    " #\"aggregation_areas\": [{\"name\": \"aggr_lvl_1\",\n",
    " #  \"file\": \"aggr_lvl_1.geojson\",\n",
    " #  \"field_name\": \"name\"},\n",
    " # {\"name\": \"aggr_lvl_2\",\n",
    " #  \"file\": \"aggr_lvl_2.geojson\",\n",
    " #  \"field_name\": \"name\"}],\n",
    " \"building_footprints\": \"OSM\",\n",
    " \"fiat_buildings_name\": \"buildings\",\n",
    " \"fiat_roads_name\": \"roads\",\n",
    " \"bfe\": {\"name\": \"bfe\",\n",
    "  \"file\": str(Path(STATIC_DATA_DIR /\"bfe.geojson\")),\n",
    "  \"field_name\": \"bfe\"},\n",
    " \"svi\": {\"name\": None,\n",
    "  \"file\": str(Path(STATIC_DATA_DIR /\"CDC_svi_2020.gpkg\")),\n",
    "  \"field_name\": \"SVI\",\n",
    "  \"threshold\": 0.5},\n",
    " \"road_width\": 5.0,\n",
    " \"return_periods\": [1, 2, 5, 10, 25, 50, 100],\n",
    " \"sfincs_overland\": {\"name\": \"overland\",\n",
    "  \"reference\": \"MSL\",\n",
    "  \"vertical_offset\": None},\n",
    " \"sfincs_offshore\": {\"name\": \"offshore\",\n",
    "  \"reference\": \"MSL\",\n",
    "  \"vertical_offset\": {\"value\": 0.33, \"units\":\"feet\"}},\n",
    " \"dem\": {\"filename\": str(Path(STATIC_DATA_DIR / \"charleston_14m.tif\")),\n",
    "  \"units\": \"meters\"},\n",
    " \"excluded_datums\": [\"NAVD88\"],\n",
    " \"slr_scenarios\": {\"file\": \"slr.csv\",\n",
    "  \"relative_to_year\": 2020},\n",
    " \"scs\": {\"file\": str(Path(STATIC_DATA_DIR / \"scs_rainfall.csv\")),\n",
    "  \"type\": \"type_3\"},\n",
    " \"tide_gauge\": {\"source\":\"noaa_coops\",\n",
    "  \"description\": \"\",\n",
    "  \"max_distance\": {\"value\": 100.0, \"units\": \"miles\"}},\n",
    " \"cyclones\": True,\n",
    " \"cyclone_basin\": \"NA\",\n",
    " \"obs_point\": [{\"name\": \"ashley_river\",\n",
    "   \"description\": \"Ashley River - James Island Expy\",\n",
    "   \"lat\": 32.7765,\n",
    "   \"lon\": -79.9543},\n",
    "  {\"name\": 8665530,\n",
    "   \"description\": \"Charleston Cooper River Entrance\",\n",
    "   \"ID\": 8665530,\n",
    "   \"lat\": 32.78,\n",
    "   \"lon\": -79.9233}],\n",
    " \"probabilistic_set\": \"test_set\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb2f83a",
   "metadata": {},
   "source": [
    "When we work with a dictionary, we need to save the configuration file as a .toml-file, so that we can call it later when we run the Database Builder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dce0597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the configuration file\n",
    "with open(Path(STATIC_DATA_DIR /\"db_config_advanced.toml\"), \"w\") as f:\n",
    "    toml.dump(config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f7db67",
   "metadata": {},
   "source": [
    "## üèÉ‚Äç‚ôÄÔ∏è **Step 5**. Run the Database Builder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102d30e2",
   "metadata": {},
   "source": [
    "We are ready to run the Database Builder with the configuration that we just created above. First we are going to run **Option 3a** - in which we generated the configuration using the FloodAdapt classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b946f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Option 2a -  DB-builder config from FloodAdapt classes\n",
    "db_build = db.DatabaseBuilder(config_model)\n",
    "db_build.build(overwrite= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fa2378",
   "metadata": {},
   "source": [
    "Now we are going to run **Option 3b** - in which we manually created a configuration file from a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa883c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model dir already exists and files might be overwritten: C:\\Users\\rautenba\\repos\\FloodAdapt\\docs\\_data\\examples\\static-data\\1_DatabaseBuilder\\Database\\charleston_example_advanced2\\static\\templates\\fiat\\exposure.\n",
      "Model dir already exists and files might be overwritten: C:\\Users\\rautenba\\repos\\FloodAdapt\\docs\\_data\\examples\\static-data\\1_DatabaseBuilder\\Database\\charleston_example_advanced2\\static\\templates\\fiat\\vulnerability.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Path scs_rainfall.csv does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m config = db.ConfigModel.read(config_path)\n\u001b[32m      4\u001b[39m dbs = db.DatabaseBuilder(config)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mdbs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\repos\\FloodAdapt\\flood_adapt\\database_builder\\database_builder.py:434\u001b[39m, in \u001b[36mDatabaseBuilder.build\u001b[39m\u001b[34m(self, overwrite)\u001b[39m\n\u001b[32m    431\u001b[39m \u001b[38;5;28mself\u001b[39m.setup()\n\u001b[32m    433\u001b[39m \u001b[38;5;66;03m# Prepare site configuration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m site = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_site_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    435\u001b[39m site.save(\u001b[38;5;28mself\u001b[39m.static_path / \u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33msite.toml\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    437\u001b[39m \u001b[38;5;66;03m# Add infometric and infographic configurations\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\repos\\FloodAdapt\\flood_adapt\\database_builder\\database_builder.py:1512\u001b[39m, in \u001b[36mDatabaseBuilder.create_site_config\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1497\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_site_config\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Site:\n\u001b[32m   1498\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create the site configuration for the FloodAdapt model.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \n\u001b[32m   1500\u001b[39m \u001b[33;03m    The order of these functions is important!\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1510\u001b[39m \n\u001b[32m   1511\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1512\u001b[39m     sfincs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_sfincs_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1513\u001b[39m     \u001b[38;5;28mself\u001b[39m.add_probabilistic_set()\n\u001b[32m   1514\u001b[39m     fiat = \u001b[38;5;28mself\u001b[39m.create_fiat_model()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\repos\\FloodAdapt\\flood_adapt\\database_builder\\database_builder.py:1166\u001b[39m, in \u001b[36mDatabaseBuilder.create_sfincs_config\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1158\u001b[39m config = \u001b[38;5;28mself\u001b[39m.create_sfincs_model_config()\n\u001b[32m   1159\u001b[39m tide_gauge = \u001b[38;5;28mself\u001b[39m.create_tide_gauge()\n\u001b[32m   1161\u001b[39m sfincs = SfincsModel(\n\u001b[32m   1162\u001b[39m     config=config,\n\u001b[32m   1163\u001b[39m     water_level=\u001b[38;5;28mself\u001b[39m.water_level_references,\n\u001b[32m   1164\u001b[39m     slr_scenarios=\u001b[38;5;28mself\u001b[39m.create_slr(),\n\u001b[32m   1165\u001b[39m     dem=\u001b[38;5;28mself\u001b[39m.create_dem_model(),\n\u001b[32m-> \u001b[39m\u001b[32m1166\u001b[39m     scs=\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_scs_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1167\u001b[39m     cyclone_track_database=\u001b[38;5;28mself\u001b[39m.create_cyclone_track_database(),\n\u001b[32m   1168\u001b[39m     tide_gauge=tide_gauge,\n\u001b[32m   1169\u001b[39m     river=\u001b[38;5;28mself\u001b[39m.create_rivers(),\n\u001b[32m   1170\u001b[39m     obs_point=\u001b[38;5;28mself\u001b[39m.create_observation_points(),\n\u001b[32m   1171\u001b[39m )\n\u001b[32m   1173\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sfincs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\repos\\FloodAdapt\\flood_adapt\\database_builder\\database_builder.py:1201\u001b[39m, in \u001b[36mDatabaseBuilder.create_scs_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.scs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1200\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1201\u001b[39m scs_file = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_exists_and_absolute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1202\u001b[39m db_scs_file = \u001b[38;5;28mself\u001b[39m.static_path / \u001b[33m\"\u001b[39m\u001b[33mscs\u001b[39m\u001b[33m\"\u001b[39m / scs_file.name\n\u001b[32m   1203\u001b[39m db_scs_file.parent.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\repos\\FloodAdapt\\flood_adapt\\database_builder\\database_builder.py:1852\u001b[39m, in \u001b[36mDatabaseBuilder._check_exists_and_absolute\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m   1850\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Check if the path is absolute or relative and return a Path object. Raises an error if the path is not valid.\"\"\"\u001b[39;00m\n\u001b[32m   1851\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(path).exists():\n\u001b[32m-> \u001b[39m\u001b[32m1852\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPath \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not exist.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1854\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m Path(path).is_absolute():\n\u001b[32m   1855\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Path(path)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Path scs_rainfall.csv does not exist."
     ]
    }
   ],
   "source": [
    "# Run Option 2b -  DB-builder config from dictionary\n",
    "config_path = STATIC_DATA_DIR  / \"db_config_advanced.toml\"\n",
    "config = db.ConfigModel.read(config_path)\n",
    "dbs = db.DatabaseBuilder(config)\n",
    "dbs.build(overwrite = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c341277e",
   "metadata": {},
   "source": [
    "## üöÄ **Step 6.** - Reading-in the FloodAdapt database\n",
    "Now that we built the database we can open it and continue to work with it.  \n",
    "\n",
    "In the other example notebooks in this repository you can find the instructions on how to create and save the single components to create a full scenario (events, measures, strategies, projections) in your database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cf9069",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = Settings(\n",
    "    DATABASE_ROOT=Path(STATIC_DATA_DIR / \"Database\").resolve(),\n",
    "    DATABASE_NAME=\"charleston_example_advanced1\"\n",
    ")\n",
    "fa = FloodAdapt(database_path=settings.database_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34537b5",
   "metadata": {},
   "source": [
    "## **Finished!**\n",
    "\n",
    "Congratulations! You created your own FloodAdapt database and know now how to initiate it. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
