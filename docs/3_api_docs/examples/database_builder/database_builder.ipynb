{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# üìò Example: Database Builder with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate how to use the **Database builder** API of FloodAdapt to build a FloodAdapt database in a new location! For this, we will use an example area in Charleston, South Carolina, for which we have already generated a SFINCS and a Delft-FIAT model.\n",
    "\n",
    "In order to use the **DatabaseBuilder** of FloodAdapt a set of **configuration** parameters are needed. The **configuration** parameters can be divided to **mandatory** and **optional** ones. Using only the mandatory parameters (i.e., baseline FloodAdapt configuration) will result in a simple but functional version of FloodAdapt. By adding optional parameters to your configuration, you can create a more advanced FloodAdapt database with additional features. If you want to learn more about the configuration parameters, please refer to the [Database-Builder](../../../4_system_setup/database.qmd) of the Setup Guide in the documentation.\n",
    "\n",
    "The configuration can be either created through available FloodAdapt classes or can be parsed as a simple dictionary. We advice you to work with the FloodAdapt classes, since this can avoid using wrong parameter names or values with the help of type hinting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import toml\n",
    "from pathlib import Path\n",
    "from hydromt_fiat.fiat import FiatModel\n",
    "from hydromt_sfincs.sfincs import SfincsModel\n",
    "import flood_adapt.database_builder as db\n",
    "from flood_adapt import FloodAdapt\n",
    "from flood_adapt import Settings\n",
    "from flood_adapt import unit_system as us\n",
    "from flood_adapt.config.sfincs import FloodModel\n",
    "from flood_adapt.objects.forcing.tide_gauge import TideGaugeSource\n",
    "from flood_adapt.config.sfincs import ObsPointModel\n",
    "from flood_adapt.config.sfincs import SlrScenariosModel\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "The study area is in **Charleston, South Carolina**, a coastal city on the East Coast of the United States. To run this notebook, we have already prepared a SFINCS model and a Delft-FIAT model for this area. Both these models are meant for demonstration purposes only. \n",
    "\n",
    "In this notebook we will go through all the mandatory and optional configuration parameters to create a database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Mandatory configuration parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## üíæ Database save path\n",
    "\n",
    "First, we need to define the path where the database will be saved. This is done by defining the `database_path` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the static data folder\n",
    "STATIC_DATA_DIR = Path(\"../../../_data/examples/static-data/1_DatabaseBuilder\").resolve()\n",
    "\n",
    "database_path=(STATIC_DATA_DIR / \"Database\").as_posix()  # Where the database will be stored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## üåê Overland SFINCS model\n",
    "\n",
    "One of the mandatory inputs for a FloodAdapt database is an overland SFINCS model. Let's first inspect the extents of our overland SFINCS model, by loading the model with the HydroMT-SFINCS plugin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path of the SFINCS overland model\n",
    "fn_sfincs = STATIC_DATA_DIR  / \"overland\"\n",
    "# Use HydroMT-SFINCS to read the SFINCS model\n",
    "sfincs = SfincsModel(root=str(fn_sfincs), mode=\"r\")\n",
    "sfincs.read()\n",
    "# Get the extent of the SFINCS model\n",
    "gdf = sfincs.region[[\"geometry\"]]\n",
    "gdf[\"name\"] = \"SFINCS Model Extent\"\n",
    "# Make a map of the SFINCS model extent\n",
    "gdf.explore(\n",
    "    style_kwds={\"fillColor\": \"blue\", \"color\": \"black\", \"weight\": 1, \"fillOpacity\": 0.2},\n",
    "    tiles=\"CartoDB positron\",\n",
    "    column=\"name\",\n",
    "    legend=True,\n",
    "    legend_kwds={\"caption\": \"Region\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "The SFINCS model is specified using the `sfincs_overland` attribute, which is a `FloodModel` class that includes the path to the SFINCS model, defined by the attribute `name` and the vertical reference that the model has, defined by `reference`. The SFINCS model was build with elevation data in the **NAVD88** vertical reference system, so we set the `reference` to `NAVD88`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the overland SFINCS model path and vertical reference\n",
    "sfincs_overland=FloodModel(\n",
    "    name=(STATIC_DATA_DIR / \"overland\").as_posix(),\n",
    "    reference=\"NAVD88\" \n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## üè† Delft-FIAT model\n",
    "\n",
    "Another mandatory input is the Delft-FIAT model. We can inspect the exposure objects (buildings and roads) of the Delft-FIAT model, by loading the model with the HydroMT-FIAT plugin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path of the FIAT model\n",
    "fn_fiat = STATIC_DATA_DIR  / \"fiat\"\n",
    "# Read the FIAT model using HydroMT-FIAT\n",
    "fiat = FiatModel(root=str(fn_fiat), mode=\"r\")\n",
    "fiat.read()\n",
    "# Get the geodataframe with exposure data\n",
    "gdf = fiat.exposure.get_full_gdf(fiat.exposure.exposure_db)\n",
    "# Plot the region and the secondary_object_types of the exposure data\n",
    "gdf.explore(column=\"primary_object_type\", \n",
    "                name=\"Exposure types\",\n",
    "                tiles=\"CartoDB positron\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "The Delft-FIAT model is simply specified using the `fiat` attribute, which points to the path of the Delft-FIAT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the FIAT model path\n",
    "fiat=(STATIC_DATA_DIR / \"fiat\").as_posix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## üìè Define the unit system\n",
    "\n",
    "Then, a `unit_system` needs to be specified, which can be either `metric` or `imperial`. The unit_system will determine the default units used in the database. Since for our example we are in U.S., we will use the `imperial` unit system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the unit system for the database\n",
    "unit_system=db.UnitSystems.imperial "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è GUI map color scaling\n",
    "\n",
    "For the output visualizations, scaling values need to be specified for each output type, using the `gui` attribute. These values will be used to scale the colors of the map visualizations in the GUI. There are four types of outputs that can be visualized in the GUI: **flood_depth, aggregated damages, footprint damages, and benefits**. For each of these outputs, we can specify the maximum values for the color scaling using the `max_flood_depth`, `max_aggr_dmg`, `max_footprint_dmg`, and `max_benefits` attributes, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the max values for each type of layer in the GUI\n",
    "gui=db.GuiConfigModel(\n",
    "    max_flood_depth=5,\n",
    "    max_aggr_dmg=1e6,\n",
    "    max_footprint_dmg=250000,\n",
    "    max_benefits=5e6,\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "# Additional configuration parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## üé≤ Probabilistic event set and risk calculations\n",
    "\n",
    "We can add a probabilistic event set by providing the path to an existing event set with the attribute `probabilistic_set`. This will enable us to run risk and benefit scenarios in FloodAdapt (see [Risk and benefit analysis](../../../4_system_setup/index.qmd#Risk-and-benefit-analysis)). \n",
    "\n",
    "In case we provide a probabilistic event set to enable risk calculations, we can also specify the return periods that will be calculated from the event set in FloodAdapt during risk scenario runs. The default values are [1, 2, 5, 10, 25, 50, 100] years, but you can specify any other set of values with the `return_periods` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Path to the prepared probabilistic set\n",
    "probabilistic_set = str(Path(STATIC_DATA_DIR  / \"test_set\"))\n",
    "# Here we just use the standard return periods\n",
    "return_periods = [1, 2, 5, 10, 25, 50, 100] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## üåÄ SFINCS offshore model\n",
    "\n",
    "If we have a SFINCS offshore model we can also pass this into the configuration with the `sfincs_offshore` attribute in the same way as the overland SFINCS model. This will allow us to run extra types of events (see [Simulating hurricane events and ‚Äòungauged‚Äô historical events](../../../4_system_setup/index.qmd#Simulating-hurricane-events-and-'ungauged'-historical-events)). Let's first visualize the SFINCS offshore model to see its extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HydroMT-SFINCS to read the SFINCS model\n",
    "off_sfincs_path = (STATIC_DATA_DIR / \"offshore\").as_posix()\n",
    "sfincs = SfincsModel(root=off_sfincs_path, mode=\"r\")\n",
    "sfincs.read()\n",
    "# Get the extent of the SFINCS model\n",
    "gdf = sfincs.region[[\"geometry\"]]\n",
    "gdf[\"name\"] = \"offshore SFINCS Model Extent\"\n",
    "# Make a map of the SFINCS model extent\n",
    "gdf.explore(\n",
    "    style_kwds={\"fillColor\": \"blue\", \"color\": \"black\", \"weight\": 1, \"fillOpacity\": 0.2},\n",
    "    tiles=\"CartoDB positron\",\n",
    "    column=\"name\",\n",
    "    legend=True,\n",
    "    legend_kwds={\"caption\": \"Region\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "Similarly, to the onshore SFINCS model, we can use a FloodModel class to define the path with the attribute `name` and its vertical reference with the attribute `reference` (which for an offshore models is typically 'MSL'). In case a correction is needed to bring MSL to present day conditions (see [Sea level offset for offshore simulations](../../../2_technical_docs/EventScenario.qmd#Sea-level-offset-for-offshore-simulations)), the `vertical_offset` attribute can be used to specify the correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the SFINCS offshore model\n",
    "sfincs_offshore=FloodModel(\n",
    "    name=off_sfincs_path,\n",
    "    reference=\"MSL\",\n",
    "    vertical_offset=us.UnitfulLength(\n",
    "        value=0.33, units=us.UnitTypesLength.feet # in this case we found from observations that there is an offset of 0.33 feet\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## üå™Ô∏è Historical hurricanes\n",
    "\n",
    "If we have an offshore SFINCS model, we can run historical hurricanes as well if we are in a hurricane prone area. The configuration for running hurricanes or not, is set with the `cyclones` attribute, which in case we are in an area were hurricanes are not relevant we could turn to `False`. If this is set to `True` (which is the default value), the `cyclone_basin` attribute can be used to define the oceanic basin. The `Basins` class can be used to check the available basins. In the case of Charleston we are going to use `NA` - for North Atlantic. If this is not specified, all global basins will be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cyclone tracks\n",
    "cyclones=True\n",
    "cyclone_basin=db.Basins.NA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## üïí  Tide gauge data\n",
    "\n",
    "If there are water level observations from a close by tide gauge we can add them in the database, so they can directly be used during event creation (see [Downloading historical water levels](../../../4_system_setup/index.qmd#downloading-historical-water-levels)), by using the `tide_gauge` attribute. \n",
    "\n",
    "With the `source` attribute assigned to `file`, and the use of the `file` attribute to define the path to a csv file with the tide gauge data, we can directly use the tide gauge data in the database. The vertical reference of the tide gauge data can be defined by the `ref` attribute. The CSV file should have two columns; the first contains a ‚Äòdatetime‚Äô in the format DD/MM/YYYY HH:MM and the second column contains the water levels relative to the vertical reference defined.\n",
    "\n",
    "In U.S., instead of manually providing a file, we can choose `db.TideGaugeSource.noaa_coops` as the `source` attribute, to find the closest tide gauge from the **NOOAA COOPS** tide gauge network. To avoid using a stations that is really far away, we can also specify a `max_distance` attribute, which will be used to filter the stations. If no station is found within the specified distance, the tide gauge data will not be added to the database. A set of water level references from this station will be added to the database as well. These include **\"MLLW\", \"MHHW\", \"NAVD88\", \"MSL\"**. The default reference of the observation is `MLLW`, which can be changed with the `ref` attribute.\n",
    "\n",
    "In our case we will use the NOAA COOPS tide gauge data with a limit of 100 miles and we will keep the default reference of `MLLW`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add tide gauge\n",
    "tide_gauge=db.TideGaugeConfigModel(\n",
    "    source=TideGaugeSource.noaa_coops,\n",
    "    max_distance=us.UnitfulLength(\n",
    "        value=100, units=us.UnitTypesLength.miles\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## üìç Observation points\n",
    "\n",
    "By using the attribute `obs_points` we can add a list of observation points for which we will extract timeseries of water levels as an output of our event scenarios. We can add a list of `ObsPointModel` objects. Each of these objects must have a `name` and a `lat` and `lon` attribute. The `description` is optional.\n",
    "\n",
    "Keep in mind that if a tide gauge station is added to the database, it will be automatically added as an observation point as well if it falls within the overland SFINCS domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add observation points\n",
    "obs_point=[\n",
    "    ObsPointModel(\n",
    "        name=\"Ashley_river\",\n",
    "        description=\"Ashley River - James Island Expy\",\n",
    "        lat=32.7765,\n",
    "        lon=-79.9543,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## üìà Sea Level Rise (SLR) scenarios\n",
    "\n",
    "We can add sea level rise scenarios to be used in the projections of FloodAdapt, by using the `slr_scenarios` attribute, which should be a `SlrScenariosModel` object, with a `file` attribute pointing to a csv file with the columns:  **year, unit, scenario_1, scenario_2, ..., scenario_n**, and a `relative_to_year` attribute, which indicate the year relative to which these scenarios should be translated, when used in FloodAdapt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Here we have created a slr scenario csv file like this already. Let's have a quick look in what the csv file looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "slr_csv = (STATIC_DATA_DIR  / \"slr.csv\").as_posix()\n",
    "pd.read_csv(slr_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add SLR scenarios\n",
    "slr_scenarios=SlrScenariosModel(\n",
    "    file=slr_csv,\n",
    "    relative_to_year=2020,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "## üßç‚Äç‚ôÇÔ∏èüßç‚Äç‚ôÄÔ∏è Social Vulnerability Index (SVI)\n",
    "\n",
    "A social vulnerability (SVI) layer can be added to the database for extra infographics related to who is impacted. This can be done with the `svi` attribute which is a `db.SviConfigModel` object. The path to a geospatial file with the SVI layer is provided with the `file` attribute, the `field_name` attribute defines the column name within the spatial file with the SVI value and the `threshold` defines the threshold value for the SVI, which distinguishes between vulnerable and non-vulnerable areas. \n",
    "\n",
    "In our case we have already clipped an SVI layer (from https://www.atsdr.cdc.gov/place-health/php/svi/svi-data-documentation-download.html) to the Charleston area, so we can use it directly. Let's have a quick look in what the SVI layer looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "svi_path = (STATIC_DATA_DIR / \"svi.gpkg\").as_posix()\n",
    "svi_layer = gpd.read_file(svi_path)\n",
    "# Make a map of the SVI layer\n",
    "svi_layer.explore(\n",
    "    column=\"SVI\",\n",
    "    name=\"Social Vulnerability Index (SVI)\",\n",
    "    tiles=\"CartoDB positron\",\n",
    "    cmap=\"RdBu_r\",\n",
    "    scheme=None,\n",
    "    style_kwds={\"color\": \"black\", \"weight\": 0.5, \"fillOpacity\": 0.7},\n",
    "    legend=True,\n",
    "    legend_kwds={\"caption\": \"SVI (0.5=center)\"},\n",
    "    categorical=False,\n",
    "    center=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "Now, let's create the SVI configuration object, using the `SviConfigModel` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add social vulnerability index\n",
    "svi=db.SviConfigModel(\n",
    "    file=svi_path,\n",
    "    field_name=\"SVI\",\n",
    "    threshold=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## üåä Base Flood Elevation (BFE)\n",
    "\n",
    "A base flood elevation (BFE) can be added to the database which allows users to elevate homes relative to this layer. This can be done with the `bfe` attribute which is a `db.SpatialJoinModel` object. The path to the geospatial vector file with the BFE layer is provided with the `file` attribute, the `field_name` attribute defines the column name within the spatial file with the BFE value.\n",
    "\n",
    "In our case we have already created some dummy data, so we can use it directly. Let's have a quick look in what the BFE layer looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "bfe_path = (STATIC_DATA_DIR / \"bfe.gpkg\").as_posix()\n",
    "bfe_layer = gpd.read_file(bfe_path)\n",
    "# Make a map of the BFE layer\n",
    "bfe_layer_valid = bfe_layer[bfe_layer[\"STATIC_BFE\"] != -9999]\n",
    "bfe_layer_valid.explore(\n",
    "    column=\"STATIC_BFE\",\n",
    "    name=\"Base Flood Elevation (BFE) - feet\",\n",
    "    tiles=\"CartoDB positron\",\n",
    "    cmap=\"Blues\",\n",
    "    scheme=None,\n",
    "    style_kwds={\"color\": \"black\", \"weight\": 0.5, \"fillOpacity\": 0.7},\n",
    "    legend=True,\n",
    "    categorical=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "Let's create the BFE configuration object, using the `SpatialJoinModel` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add base flood elevation\n",
    "bfe=db.SpatialJoinModel(\n",
    "    file=bfe_path,\n",
    "    name=\"bfe\",\n",
    "    field_name=\"STATIC_BFE\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "## üî≤ Aggregation areas\n",
    "\n",
    "A list of different types of aggregation levels that can be included in the metrics aggregation can be defined by the `aggregation_areas` attribute. Each entry in the list can be defined by the `SpatialJoinModel` class, which has a `name` attribute to specify the shown name, a `file` attribute to specify the location of the geospatial vector file. The `name` is used to identify the aggregation area in the database, while the `file` is the path to a geospatial vector file with the aggregation area polygons and a `field_name` attribute to specify the column name within the spatial file with the aggregation area names.\n",
    "\n",
    "Keep in mind that the aggregation areas specified in the database builder configuration are additional to any aggregation areas already defined in the Delft-FIAT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggr_path = (STATIC_DATA_DIR / \"subdivision.gpkg\").as_posix()\n",
    "# Here we can visualize in a map one of the aggragetion levels\n",
    "aggr = gpd.read_file(aggr_path)\n",
    "# Make a map of the aggregation level 1 layer\n",
    "aggr.explore(\n",
    "    column=\"SUDIV_UNIQUE\",\n",
    "    name=\"Aggregation Level: Subdivision\",\n",
    "    tiles=\"CartoDB positron\",\n",
    "    style_kwds={\"color\": \"black\", \"weight\": 0.5, \"fillOpacity\": 0.7},\n",
    "    legend=True,\n",
    "    legend_kwds={\"caption\": \"Aggregation Level 1\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "We can now add the aggregation level to the configuration, using the `SpatialJoinModel` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add aggregation areas\n",
    "aggregation_areas=[\n",
    "db.SpatialJoinModel(\n",
    "   name=\"Subdivision\",\n",
    "   file=aggr_path,\n",
    "   field_name=\"SUDIV_UNIQUE\",\n",
    ")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "## üèòÔ∏èüõ£Ô∏è Exposure Objects types\n",
    "\n",
    "The location of the exposure objects (buildings and roads) in Delft-FIAT is defined by different geospatial vector files. By default, the name of the buildings file is set to `buildings` and the roads file is set to `roads`. If this is not the case these values can be changed using the `fiat_buildings_name` and `fiat_roads_name` attributes, respectively.\n",
    "\n",
    "FloodAdapt works with roads as polygons, so if the `roads` file is in line format, we can use the `road_width` attribute to define the width of the road. This will be used to create a polygon representation of the roads in the database. By default this is set to `5.0` meters, but it can be changed to any other value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In our case we keep the default values for the exposure options\n",
    "fiat_buildings_name=\"buildings\"\n",
    "fiat_roads_name=\"roads\"\n",
    "road_width= us.UnitfulLength(value=5, units=us.UnitTypesLength.meters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "## üè¢ Building Footprints\n",
    "\n",
    "Buildings in FIAT exposure can be represent as points. Then we want to aggregate these to building footprints, which are polygons. This can be done by using the `building_footprints` attribute, which is a `db.SpatialJoinModel` object. The `file` attribute defines the path to the geospatial vector file with the building footprints and the `field_name` attribute defines the column name within the spatial file with the building footprint names. If we don't provide a building footprints file, the building footprints will be downloaded from the OpenStreetMap (OSM) database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize building footprints in a single color\n",
    "building_fp_path = (STATIC_DATA_DIR / \"FEMA_building_footprints.gpkg\").as_posix()\n",
    "building_fp_gdf = gpd.read_file(building_fp_path)\n",
    "building_fp_gdf.explore(\n",
    "    name=\"Building Footprints\",\n",
    "    tiles=\"CartoDB positron\",\n",
    "    style_kwds={\"color\": \"black\", \"weight\": 0.5, \"fillColor\": \"#3182bd\", \"fillOpacity\": 0.7},\n",
    "    legend=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "We can now define the building footprints configuration object, using the `SpatialJoinModel` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "building_footprints = db.SpatialJoinModel(\n",
    "    file=building_fp_path,\n",
    "    field_name=\"BUILD_ID\", # unique identifier for the building footprints\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "# Create the database configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "Now, that we created all the individual objects we can compile them in a `ConfigModel` object. As mentioned using only the mandatory attributes we can already build a functional FloodAdapt database. However, we can also add the optional attributes to create a more advanced FloodAdapt database. Here we are going to create two configurations, one with only the mandatory attributes and one with all the optional attributes as well.\n",
    "\n",
    "A mandatory attribute is the `name` of the database, which will be the unique identifier of the database. The `database_path` attribute defines the path where these database will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile ConfigModel with only the mandatory attributes\n",
    "config_model_basic = db.ConfigModel(\n",
    "        name = \"charleston_example_basic\",\n",
    "        database_path=database_path,\n",
    "        unit_system=unit_system,  \n",
    "        gui=gui, \n",
    "        sfincs_overland=sfincs_overland,\n",
    "        fiat=fiat, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile ConfigModel with all the optional attributes\n",
    "config_model_advanced = db.ConfigModel(\n",
    "        name = \"charleston_example_advanced\",\n",
    "        database_path=database_path,\n",
    "        unit_system=unit_system,  \n",
    "        gui=gui, \n",
    "        sfincs_overland=sfincs_overland,\n",
    "        fiat=fiat, \n",
    "        probabilistic_set=probabilistic_set,\n",
    "        return_periods=return_periods,\n",
    "        sfincs_offshore=sfincs_offshore,\n",
    "        slr_scenarios=slr_scenarios,\n",
    "        tide_gauge=tide_gauge,\n",
    "        cyclones=cyclones,\n",
    "        cyclone_basin=cyclone_basin,\n",
    "        obs_point=obs_point,\n",
    "        aggregation_areas=aggregation_areas,\n",
    "        building_footprints=building_footprints,\n",
    "        fiat_buildings_name=fiat_buildings_name,\n",
    "        fiat_roads_name=fiat_roads_name,\n",
    "        bfe=bfe,\n",
    "        svi=svi,\n",
    "        road_width=road_width,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "# Run the Database Builder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "We are now ready to run the Database Builder with the configurations that we just created above. When running the database builder, all the steps and inputs that are used are logged and printed to the console. Moreover, a log file is saved in the database path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "Let's first create the basic database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.create_database(config=config_model_basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "Now, we can create the advanced database with all the optional attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.create_database(config=config_model_advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "# Reading-in the FloodAdapt database\n",
    "Now that we built the database we can open it and continue to work with it.  \n",
    "\n",
    "Let's open the advanced database that we just created and verify it can be loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = Settings(\n",
    "    DATABASE_ROOT=Path(STATIC_DATA_DIR / \"Database\").resolve(),\n",
    "    DATABASE_NAME=\"charleston_example_advanced\"\n",
    ")\n",
    "fa = FloodAdapt(database_path=settings.database_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fa_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
