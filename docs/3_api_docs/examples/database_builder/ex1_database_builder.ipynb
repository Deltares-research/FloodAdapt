{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# üìò Example: Database Builder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate how to use the **Database builder** API of FloodAdapt, which greatly simplifies the process of setting up a FloodAdapt database in a new location!\n",
    "\n",
    "The most critical components of a FloodAdapt database are the [**SFINCS**](https://github.com/Deltares/SFINCS) and [**Delft-FIAT**](https://github.com/Deltares/Delft-FIAT) models, both of which can be generated with great ease using the [**HydroMT-SFINCS**](https://deltares.github.io/hydromt_sfincs/latest/) and the [**HydroMT-FIAT**](https://deltares.github.io/hydromt_fiat/stable/) plugins of [**HydroMT**](https://deltares.github.io/hydromt/latest/).\n",
    "\n",
    "For this notebook, we will use an example area in Charleston, USA, for which we have already generated a SFINCS and a Delft-FIAT model.\n",
    "\n",
    "In order to use the **DatabaseBuilder** of FloodAdapt a set of **configuration** parameters are needed. The **configuration** parameters can be divided to **mandatory** and **optional** ones. Using only the mandatory parameters (i.e., baseline FloodAdapt configuration) will result in a simple but functional version of FloodAdapt. By adding optional parameters to your configuration, you can create a more advanced FloodAdapt database with additional features. \n",
    "\n",
    "If you want to learn more about the configuration parameters, please refer to the [Database-Builder](../../../4_system_setup/database.qmd) of the Setup Guide in the documentation.\n",
    "\n",
    "The configuration can be either created through available FloodAdapt classes or can be parsed as a simple dictionary. We advice you to work with the FloodAdapt classes, since this can avoid using wrong parameter names or values with the help of type hinting. \n",
    "\n",
    "In this notebook we will start by creating a simple FloodAdapt database using the baseline FloodAdapt configuration ([Step-2: Build a basic FloodAdapt database](#-step-2-build-a-basic-floodadapt-database)), and then we will go through all the optional configuration parameters and create a more complex database ([Step-3: Build an advanced FloodAdapt Database](#Ô∏è-step-3-build-an-advanced-floodadapt-model))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import toml\n",
    "from pathlib import Path\n",
    "from hydromt_fiat.fiat import FiatModel\n",
    "from hydromt_sfincs.sfincs import SfincsModel\n",
    "import flood_adapt.database_builder as db\n",
    "from flood_adapt import FloodAdapt\n",
    "from flood_adapt import Settings\n",
    "from flood_adapt import unit_system as us\n",
    "from flood_adapt.config.sfincs import FloodModel\n",
    "from flood_adapt.objects.forcing.tide_gauge import TideGaugeSource\n",
    "from flood_adapt.config.sfincs import ObsPointModel\n",
    "from flood_adapt.config.sfincs import SlrScenariosModel\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## üîç **Step 1**. Explore the SFINCS and Delft-FIAT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "The study area is in **Charleston, USA**, a coastal city on the East Coast of the United States. To run this notebook, we have already prepared a SFINCS model and a Delft-FIAT model for this area. Both these models are simplified and are used for demonstration purposes only.\n",
    "\n",
    "We can first inspect the extents of our SFINCS model, by loading the model with the HydroMT-SFINCS plugin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the static data folder\n",
    "STATIC_DATA_DIR = Path(\"../../../_data/examples/static-data/1_DatabaseBuilder\").resolve()\n",
    "# Get the path of the SFINCS overland model\n",
    "fn_sfincs = STATIC_DATA_DIR  / \"overland\"\n",
    "# Use HydroMT-SFINCS to read the SFINCS model\n",
    "sfincs = SfincsModel(root=str(fn_sfincs), mode=\"r\")\n",
    "sfincs.read()\n",
    "# Get the extent of the SFINCS model\n",
    "gdf = sfincs.region[[\"geometry\"]]\n",
    "gdf[\"name\"] = \"SFINCS Model Extent\"\n",
    "# Make a map of the SFINCS model extent\n",
    "gdf.explore(\n",
    "    style_kwds={\"fillColor\": \"blue\", \"color\": \"black\", \"weight\": 1, \"fillOpacity\": 0.2},\n",
    "    tiles=\"CartoDB positron\",\n",
    "    column=\"name\",\n",
    "    legend=True,\n",
    "    legend_kwds={\"caption\": \"Region\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Then we can inspect the exposure objects (buildings and roads) of the Delft-FIAT model, by loading the model with the HydroMT-FIAT plugin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path of the FIAT model\n",
    "fn_fiat = STATIC_DATA_DIR  / \"fiat\"\n",
    "# Read the FIAT model using HydroMT-FIAT\n",
    "fiat = FiatModel(root=str(fn_fiat), mode=\"r\")\n",
    "fiat.read()\n",
    "# Get the geodataframe with exposure data\n",
    "gdf = fiat.exposure.get_full_gdf(fiat.exposure.exposure_db)\n",
    "# Plot the region and the secondary_object_types of the exposure data\n",
    "gdf.explore(column=\"secondary_object_type\", \n",
    "                name=\"Exposure types\",\n",
    "                tiles=\"CartoDB positron\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## üìÑ **Step 2**: Build a **basic** FloodAdapt Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "In this step we will create a basic FloodAdapt database, using only the mandatory configuration parameters.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### üìö **Step 2a**: Build the configuration from the **Database Builder classes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "To create the configuration object we can use the `ConfigModel` class of the Database Builder.\n",
    "\n",
    "The mandatory configuration attributes include the `name` of the database and the `database_path` which points to the location where the database will be stored. Then, a `unit_system` needs to be specified, which can be either `metric` or `imperial`. The `unit_system` will determine the default units used in the database. For the output visualizations, scaling values need to be specified for each output type, using the `gui` attribute. \n",
    "\n",
    "Last, the overland SFINCS model and the Delft-FIAT model need to be specified. The SFINCS model is specified using the `sfincs_overland` attribute, which includes the path to the SFINCS model, defined by the attribute `name` and the vertical reference that the model has, defined by `reference`. The Delft-FIAT model is specified using the `fiat` attribute, which points to the path of the Delft-FIAT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_path=(STATIC_DATA_DIR / \"Database\").as_posix()  # Where the database will be stored\n",
    "unit_system=db.UnitSystems.imperial # Define the unit system for the database\n",
    "gui=db.GuiConfigModel(\n",
    "    max_flood_depth=5,\n",
    "    max_aggr_dmg=1e6,\n",
    "    max_footprint_dmg=250000,\n",
    "    max_benefits=5e6,\n",
    ")  # Define the max values for each type of layer in the GUI\n",
    "sfincs_overland=FloodModel(\n",
    "    name=(STATIC_DATA_DIR / \"overland\").as_posix(),\n",
    "    reference=\"MSL\"  # This is the vertical reference for the SFINCS model\n",
    ")  # Define the overland SFINCS model path and vertical reference\n",
    "fiat=(STATIC_DATA_DIR / \"fiat\").as_posix()  # Define the FIAT model path\n",
    "\n",
    "config_model = db.ConfigModel(\n",
    "    name=\"charleston_example_basic\", # unique name for the database\n",
    "    database_path=database_path,\n",
    "    unit_system=unit_system,\n",
    "    gui=gui,\n",
    "    sfincs_overland=sfincs_overland,\n",
    "    fiat=fiat\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### üìñ **Step 2b**: Create a configuration file from a **dictionary**\n",
    "\n",
    "An alternative approach, would be to create a dictionary with all the attributes and save it to a TOML file. Any path that is included in the file should be either an absolute path (using forward slashes `/`) or a relative path (using forward slashes `/` and relative to the path of the TOML file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case we use relative paths, but absolute paths are also possible\n",
    "config_dict = {\n",
    "\"name\": \"charleston_example_basic\",\n",
    "\"database_path\": \"Database\",\n",
    "\"sfincs_overland\" : { \n",
    "    \"name\":\"overland\",\n",
    "    \"reference\":\"NAVD88\"\n",
    "    },\n",
    "\"fiat\" : \"fiat\",\n",
    "\"unit_system\" :\"imperial\",\n",
    "\"gui\": {\n",
    "\"max_flood_depth\": 5,\n",
    "\"max_aggr_dmg\" : 1e6,\n",
    "\"max_footprint_dmg\": 250000,\n",
    "\"max_benefits\" : 5e6}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "We can then save the configuration as a .toml file at the location of the other input data, that we defined in the dictionary as relative paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the configuration file\n",
    "config_path = STATIC_DATA_DIR  / \"db_config_basic.toml\"\n",
    "with open(config_path, \"w\") as f:\n",
    "    toml.dump(config_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "The we can use the `read()` method of the `ConfigModel` class to read the configuration from the TOML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = db.ConfigModel.read(config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "We can then verify that the two created configurations are the same, by comparing the attributes of the two configuration objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "config == config_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### üèÉ‚Äç‚ôÄÔ∏è **Step 2c**: Run the Database Builder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "We can then run the Database Builder using the `DatabaseBuilder` class. The `DatabaseBuilder` class takes the configuration object we created previously as input. Then we can call the `build()` method to build the database. During building all the steps of the Database Builder are logged, so we can follow the progress of the building process and since a log file is saved, we can also check the log file after the building process is finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Option 2a -  DB-builder config from FloodAdapt classes\n",
    "db_build = db.create_database(config_model, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è **Step 3**. Build an **advanced** FloodAdapt Database\n",
    "\n",
    "In the previous step we created a simple FloodAdapt database. Our simple database is limited in functionality, so in this next step we're adding more advanced configurations to expand its capabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "For the advanced example we are only going to build the configuration from the Database Builder classes. Similarly to the simple example, the configuration can be created using a dictionary or toml file as well.\n",
    "\n",
    "We are going to first construct each individual component of the configuration, and then we will combine them into a single configuration object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Probabilistic event set\n",
    "\n",
    "We can add a probabilistic event set by providing the path to an existing event set with the attribute `probabilistic_set`. This will enable us to run risk and benefit scenarios in FloodAdapt (see [Risk and benefit analysis](../../../4_system_setup/index.qmd#Risk-and-benefit-analysis)). \n",
    "\n",
    "In case we provide a probabilistic event set to enable risk calculations, we can also specify the return periods that will be calculated from the event set in FloodAdapt during risk scenario runs. The default values are [1, 2, 5, 10, 25, 50, 100] years, but you can specify any other set of values with the `return_periods` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilistic_set = str(Path(STATIC_DATA_DIR  / \"test_set\")) # Path to the prepared probabilistic set\n",
    "return_periods = [1, 2, 5, 10, 25, 50, 100] # Here we just use the standard return periods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### SFINCS offshore model\n",
    "\n",
    "If we have a SFINCS offshore model we can also pass this into the configuration with the `sfincs_offshore` attribute in the same way as the overland SFINCS model. This will allow us to run extra types of events (see [Simulating hurricane events and ‚Äòungauged‚Äô historical events](../../../4_system_setup/index.qmd#Simulating-hurricane-events-and-'ungauged'-historical-events)). Let's first visualize the SFINCS offshore model to see its extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HydroMT-SFINCS to read the SFINCS model\n",
    "off_sfincs_path = (STATIC_DATA_DIR / \"offshore\").as_posix()\n",
    "sfincs = SfincsModel(root=off_sfincs_path, mode=\"r\")\n",
    "sfincs.read()\n",
    "# Get the extent of the SFINCS model\n",
    "gdf = sfincs.region[[\"geometry\"]]\n",
    "gdf[\"name\"] = \"offshore SFINCS Model Extent\"\n",
    "# Make a map of the SFINCS model extent\n",
    "gdf.explore(\n",
    "    style_kwds={\"fillColor\": \"blue\", \"color\": \"black\", \"weight\": 1, \"fillOpacity\": 0.2},\n",
    "    tiles=\"CartoDB positron\",\n",
    "    column=\"name\",\n",
    "    legend=True,\n",
    "    legend_kwds={\"caption\": \"Region\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "Similarly, to the onshore SFINCS model, we can use a FloodModel class to define the path with the attribute `name` and its vertical reference with the attribute `reference` (which for an offshore models is typically 'MSL'). In case a correction is needed to bring MSL to present day conditions (see [Sea level offset for offshore simulations](../../../2_technical_docs/EventScenario.qmd#Sea-level-offset-for-offshore-simulations)), the `vertical_offset` attribute can be used to specify the correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the SFINCS offshore model\n",
    "sfincs_offshore=FloodModel(\n",
    "    name=off_sfincs_path,\n",
    "    reference=\"MSL\",\n",
    "    vertical_offset=us.UnitfulLength(\n",
    "        value=0.33, units=us.UnitTypesLength.feet # in this case we found from observations that there is an offset of 0.33 feet\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Historical hurricanes\n",
    "\n",
    "If we have an offshore SFINCS model, we can run historical hurricanes as well if we are in a hurricane prone area. The configuration for running hurricanes or not, is set with the `cyclones` attribute, which in case we are in an area were hurricanes are not relevant we could turn to `False`. If this is set to `True` (which is the default value), the `cyclone_basin` attribute can be used to define the oceanic basin. The `Basins` class can be used to check the available basins. In the case of Charleston we are going to use `NA` - for North Atlantic. If this is not specified, all global basins will be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cyclone tracks\n",
    "cyclones=True\n",
    "cyclone_basin=db.Basins.NA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### Tide gauge data\n",
    "\n",
    "If there are water level observations from a close by tide gauge we can add them in the database, so they can directly be used during event creation (see [Downloading historical water levels](../../../4_system_setup/index.qmd#downloading-historical-water-levels)), by using the `tide_gauge` attribute. \n",
    "\n",
    "With the `source` attribute defined to `file`, and the use of the `file` attribute to define the path to a csv file with the tide gauge data, we can directly use the tide gauge data in the database. The vertical reference of the tide gauge data can be defined by the `ref` attribute. The CSV file should have two columns; the first contains a ‚Äòdatetime‚Äô in the format DD/MM/YYYY HH:MM and the second column contains the water levels relative to the vertical reference defined.\n",
    "\n",
    "In U.S., instead of manually providing a file, we can choose `db.TideGaugeSource.noaa_coops` as the `source` attribute, to find the closest tide gauge from the **NOOAA COOPS** tide gauge network. To avoid using a stations that is really far away, we can also specify a `max_distance` attribute, which will be used to filter the stations. If no station is found within the specified distance, the tide gauge data will not be added to the database. A set of water level references from this station will be added to the database as well. These include **\"MLLW\", \"MHHW\", \"NAVD88\", \"MSL\"**. The default reference of the observation is `MLLW`, which can be changed with the `ref` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add tide gauges\n",
    "tide_gauge=db.TideGaugeConfigModel(\n",
    "    source=TideGaugeSource.noaa_coops,\n",
    "    max_distance=us.UnitfulLength(\n",
    "        value=100, units=us.UnitTypesLength.miles\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### Observation points\n",
    "\n",
    "By using the attribute `obs_points` we can add a list of observation points for which we will extract timeseries of water levels as an output of our event scenarios. We can add a list of `ObsPointModel` objects. Each of these objects must have a `name` and a `lat` and `lon` attribute. The `description` is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add observation points\n",
    "obs_point=[\n",
    "    ObsPointModel(\n",
    "        name=\"Ashley_river\",\n",
    "        description=\"Ashley River - James Island Expy\",\n",
    "        lat=32.7765,\n",
    "        lon=-79.9543,\n",
    "    ),\n",
    "    ObsPointModel(\n",
    "        name='8665530',\n",
    "        description=\"Charleston Cooper River Entrance\",\n",
    "        lat=32.78,\n",
    "        lon=-79.9233,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### Sea Level Rise (SLR) scenarios\n",
    "\n",
    "We can add sea level rise scenarios to be used in the projections of FloodAdapt, by using the `slr_scenarios` attribute, which should be a `SlrScenariosModel` object, with a `file` attribute pointing to a csv file with the columns:  **year, unit, scenario_1, scenario_2, ..., scenario_n**, and a `relative_to_year` attribute, which indicate the year relative to which these scenarios should be translated, when used in FloodAdapt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "Here we have created a slr scenario csv file like this already. Let's have a quick look in what the csv file looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "slr_csv = (STATIC_DATA_DIR  / \"slr.csv\").as_posix()\n",
    "pd.read_csv(slr_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add SLR scenarios\n",
    "slr_scenarios=SlrScenariosModel(\n",
    "    file=slr_csv,\n",
    "    relative_to_year=2020,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### Social Vulnerability Index (SVI)\n",
    "\n",
    "A social vulnerability (SVI) layer can be added to the database for extra infographics related to who is impacted. This can be done with the `svi` attribute which is a `db.SviConfigModel` object. The path to a geospatial file with the SVI layer is provided with the `file` attribute, the `field_name` attribute defines the column name within the spatial file with the SVI value and the `threshold` defines the threshold value for the SVI, which distinguishes between vulnerable and non-vulnerable areas. \n",
    "\n",
    "In our case we have already clipped an SVI layer (from https://www.atsdr.cdc.gov/place-health/php/svi/svi-data-documentation-download.html) to the Charleston area, so we can use it directly. Let's have a quick look in what the SVI layer looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "svi_path = (STATIC_DATA_DIR / \"CDC_svi_2020.gpkg\").as_posix()\n",
    "svi_layer = gpd.read_file(svi_path)\n",
    "# Make a map of the SVI layer\n",
    "svi_layer.explore(\n",
    "    column=\"SVI\",\n",
    "    name=\"Social Vulnerability Index (SVI)\",\n",
    "    tiles=\"CartoDB positron\",\n",
    "    cmap=\"RdBu_r\",\n",
    "    scheme=None,\n",
    "    style_kwds={\"color\": \"black\", \"weight\": 0.5, \"fillOpacity\": 0.7},\n",
    "    legend=True,\n",
    "    legend_kwds={\"caption\": \"SVI (0.5=center)\"},\n",
    "    categorical=False,\n",
    "    center=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "Now, let's create the SVI configuration object, using the `SviConfigModel` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add social vulnerability index\n",
    "svi=db.SviConfigModel(\n",
    "    file=svi_path,\n",
    "    field_name=\"SVI\",\n",
    "    threshold=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### Base Flood Elevation (BFE)\n",
    "\n",
    "A base flood elevation (BFE) can be added to the database which allows users to elevate homes relative to this layer. This can be done with the `bfe` attribute which is a `db.SpatialJoinModel` object. The path to the geospatial vector file with the BFE layer is provided with the `file` attribute, the `field_name` attribute defines the column name within the spatial file with the BFE value.\n",
    "\n",
    "In our case we have already created some dummy data, so we can use it directly. Let's have a quick look in what the BFE layer looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "bfe_path = (STATIC_DATA_DIR / \"bfe.geojson\").as_posix()\n",
    "bfe_layer = gpd.read_file(bfe_path)\n",
    "# Make a map of the BFE layer\n",
    "bfe_layer.explore(\n",
    "    column=\"bfe\",\n",
    "    name=\"Base Flood Elevation (BFE)\",\n",
    "    tiles=\"CartoDB positron\",\n",
    "    cmap=\"Blues\",\n",
    "    scheme=None,\n",
    "    style_kwds={\"color\": \"black\", \"weight\": 0.5, \"fillOpacity\": 0.7},\n",
    "    legend=True,\n",
    "    categorical=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "Let's create the BFE configuration object, using the `SpatialJoinModel` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add base flood elevation\n",
    "bfe=db.SpatialJoinModel(\n",
    "    file=str(Path(STATIC_DATA_DIR / \"bfe.geojson\")),\n",
    "    name=\"bfe\",\n",
    "    field_name=\"bfe\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### Aggregation areas\n",
    "\n",
    "- `aggregation_areas`: A list of dictionaries with the entries: name, file path, field_name. Aggregates the exposure into larger spatial groups o summarize impacts on larger scale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add aggregation areas\n",
    "aggregation_areas=[\n",
    "db.SpatialJoinModel(\n",
    "   name=\"aggr_lvl_1\",\n",
    "   file=(STATIC_DATA_DIR / \"aggr_lvl_1.geojson\").as_posix(),\n",
    "   field_name=\"name\",\n",
    "),\n",
    "db.SpatialJoinModel(\n",
    "   name=\"aggr_lvl_2\",\n",
    "   file=(STATIC_DATA_DIR /\"aggr_lvl_2.geojson\").as_posix(),\n",
    "   field_name=\"name\",\n",
    "),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "To refine our Delft-FIAT model we can add additional inputs or update default settings. \n",
    "\n",
    "Sometimes our Delf-FIAT model exposure is in point data By adding the `building_footprints`attribute we can download data from Open Street Map (OSM) using the `FootprintsOptions.OSM` object. e use the building footprints to create the visualizations for the impacts on the building footprint level.  \n",
    "The `fiat_buildings_name` and `fiat_roads_name` are set to *\"buildings\"* and *\"roads\"*, respectively as default. These names capture the geometry names of these assets in the Delft-FIAT model. If you specified different names for these geometry, you must change them here. By defining a value in the `road_width` attribute we can capture the realistic width of the road. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add additional Delft-FIAT parameters\n",
    "building_footprints=db.FootprintsOptions.OSM\n",
    "fiat_buildings_name=\"buildings\"\n",
    "fiat_roads_name=\"roads\"\n",
    "road_width=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "Now, that we created all the individual objects we can compile them in the `ConfigModel`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile ConfigModel\n",
    "config_model = db.ConfigModel(\n",
    "        name = \"charleston_example_advanced\",\n",
    "        database_path=(STATIC_DATA_DIR / \"Database\").as_posix(),\n",
    "        unit_system=db.UnitSystems.imperial,  \n",
    "        gui=db.GuiConfigModel(\n",
    "                max_flood_depth=5,\n",
    "                max_aggr_dmg=1e6,\n",
    "                max_footprint_dmg=250000,\n",
    "                max_benefits=5e6,\n",
    "        ), \n",
    "        sfincs_overland=FloodModel(\n",
    "                name=(STATIC_DATA_DIR / \"overland\").as_posix(),\n",
    "                reference=\"NAVD88\", \n",
    "        ),\n",
    "        fiat=(STATIC_DATA_DIR / \"fiat\").as_posix(), \n",
    "        probabilistic_set=probabilistic_set,\n",
    "        return_periods=return_periods,\n",
    "        sfincs_offshore=sfincs_offshore,\n",
    "        slr_scenarios=slr_scenarios,\n",
    "        tide_gauge=tide_gauge,\n",
    "        cyclones=cyclones,\n",
    "        cyclone_basin=cyclone_basin,\n",
    "        obs_point=obs_point,\n",
    "        # aggregation_areas=aggregation_areas,\n",
    "        building_footprints=db.FootprintsOptions.OSM,\n",
    "        fiat_buildings_name=fiat_buildings_name,\n",
    "        fiat_roads_name=fiat_roads_name,\n",
    "        bfe=bfe,\n",
    "        svi=svi,\n",
    "        road_width=road_width,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "## üèÉ‚Äç‚ôÄÔ∏è **Step 4**. Run the Database Builder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "We are ready to run the Database Builder with the configuration that we just created above. First we are going to run **Option 3a** - in which we generated the configuration using the FloodAdapt classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Option 2a -  DB-builder config from FloodAdapt classes\n",
    "db_build = db.create_database(config_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "## üöÄ **Step 5.** - Reading-in the FloodAdapt database\n",
    "Now that we built the database we can open it and continue to work with it.  \n",
    "\n",
    "In the other example notebooks in this repository you can find the instructions on how to create and save the single components to create a full scenario (events, measures, strategies, projections) in your database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = Settings(\n",
    "    DATABASE_ROOT=Path(STATIC_DATA_DIR / \"Database\").resolve(),\n",
    "    DATABASE_NAME=\"charleston_example_advanced\"\n",
    ")\n",
    "fa = FloodAdapt(database_path=settings.database_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "## **Finished!**\n",
    "\n",
    "Congratulations! You created your own FloodAdapt database and know now how to initiate it. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fa_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
